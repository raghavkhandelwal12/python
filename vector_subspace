# Vector Subspace ‚Äì Theory & Application in Data Science (with PCA Example)
## What is a Vector Subspace?
A **Vector Subspace** is a subset of a vector space that is itself a vector space using the same scalar and vector multiplication operations.
### **Formal Definition**
Let `V` be a vector space over a field `F`. A subset `W ‚äÜ V` is a subspace if:
1. `W` is non-empty.
2. For all `u, v ‚àà W`, `u + v ‚àà W`.
3. For all `v ‚àà W` and scalar `a ‚àà F`, `a * v ‚àà W`.
If these conditions are met, then `W` inherits all other vector space properties from `V`, such as: Associativity, Commutativity, Identity element, Inverse element.
##  Geometric Intuition
- A subspace can be visualized as a flat surface (line, plane) that **passes through the origin** in the parent space.
- In `R¬≤`: A line through the origin is a subspace.
- In `R¬≥`: A plane through the origin is a subspace.
###  Example in R¬≤:
```
W = { [x, 2x] | x ‚àà ‚Ñù }
```
- Contains zero vector when `x = 0` ‚Üí `[0, 0]`
- Addition of any two points is still in `W`
- Scalar multiplication stays in `W`
Hence, it's a **valid subspace of R¬≤**.
### Not a Subspace:
```
L = { [x, 2x + 1] | x ‚àà ‚Ñù }
```
- This line does **not pass through origin** ‚Üí Not a subspace.
###  Example in R¬≥:
```
W = { [x, y, 0] | x, y ‚àà ‚Ñù }
```
- Contains zero vector: `[0, 0, 0]`
- Closed under vector addition and scalar multiplication
Hence, it is a **valid subspace of R¬≥**.
##  Subspace Conditions in Data Science
### 1. **Contains the Zero Vector**
```
W = {[x, 2x] | x ‚àà R}
Let x = 0 ‚áí [0, 2√ó0] = [0, 0] ‚àà W
```
**Used in PCA**: In PCA, data is centered around the origin by subtracting the mean. Ensures principal components (directions of variance) pass through origin.
# Principal Component Analysis (PCA) ‚Äì Mathematical Walkthrough
### What is PCA?
**Principal Component Analysis (PCA)** is a linear technique for dimensionality reduction that transforms correlated features into linearly uncorrelated variables called **principal components**.
- 1st component ‚Üí direction of **maximum variance**
- Subsequent components ‚Üí orthogonal to previous ones
### Why Use PCA?
- Reduce dimensions while keeping maximum info
- Visualization and modeling simplification
- Remove redundant features
## How PCA Uses Subspaces
- The **original feature space** is a vector space.
- Each **principal component** defines a new subspace.
- **Top-k principal components** define a **k-dimensional subspace**.
- Projection onto that subspace preserves variance.
## üìê Step-by-Step PCA Example
### Input Data (2D)
```python
X = [
    [2, 0],
    [0, 2],
    [3, 1],
    [1, 3]
]
```
### Step 1: Center the Data
```python
mean = [1.5, 1.5]
X_centered = X - mean =
[
 [ 0.5, -1.5],
 [-1.5,  0.5],
 [ 1.5, -0.5],
 [-0.5,  1.5]
]
```
### Step 2: Covariance Matrix
```python
Sigma = (1/3) * [
    [5, -3],
    [-3, 5]
] = [
    [5/3, -1],
    [-1, 5/3]
]
```
### Step 3: Eigenvalues and Eigenvectors
- Œª‚ÇÅ = 8/3, eigenvector v‚ÇÅ = [1, -1]
- Œª‚ÇÇ = 2/3, eigenvector v‚ÇÇ = [1, 1]
### Step 4: Projection onto PC1 (v‚ÇÅ)
```text
Z = X_centered ‚ãÖ v‚ÇÅ = [2, -2, 2, -2]
```
##  Summary Table
| Step | Output |
|------|--------|
| Centered Data | [ [0.5, -1.5], [-1.5, 0.5], ... ] |
| Covariance Matrix | [ [5/3, -1], [-1, 5/3] ] |
| Eigenvalues | 8/3, 2/3 |
| Eigenvectors | v‚ÇÅ = [1, -1], v‚ÇÇ = [1, 1] |
| Projected Data on PC1 | [2, -2, 2, -2] |
##  Subspace Properties (Continued)
### 2. **Closure Under Addition**
```
u = [x‚ÇÅ, 2x‚ÇÅ], v = [x‚ÇÇ, 2x‚ÇÇ]
u + v = [x‚ÇÅ + x‚ÇÇ, 2(x‚ÇÅ + x‚ÇÇ)] ‚àà W
```
 **In NLP**: Word vectors added together stay in same subspace.
### 3. **Closure Under Scalar Multiplication**
```
a * [x, 2x] = [a*x, 2a*x] = [x', 2x'] ‚àà W
```
 **In ML**: Feature scaling keeps data in the same vector space.
## üî¨ Subspace Applications in Data Science
### 1. **PCA**
- Projects data into lower-dimensional subspace
- **Top eigenvectors** define the new basis of subspace
```text
Data with 100 features ‚Üí
  ‚Üí Covariance matrix ‚Üí
  ‚Üí Eigenvectors & values ‚Üí
  ‚Üí Top 2‚Äì3 eigenvectors ‚Üí
  ‚Üí Subspace of max variance ‚Üí
  ‚Üí Projected for visualization
```
### 2. **Null Space in Linear Regression**
- Detects **multicollinearity** (redundant features)
- Vectors in null space ‚Üí no effect on output
### 3. **Clustering (K-means)**
- Each cluster is approximately centered in its own **convex subspace**
##  Subspaces and Dimensionality Reduction
### What is Dimensionality Reduction?
Reducing number of features while retaining the most **important patterns**.
### Techniques:
- **PCA (Principal Component Analysis)**
- **LDA (Linear Discriminant Analysis)**
- **Factor Analysis**
###  Why Subspace?
- Think of high-dimensional data ‚Üí as points in a large space.
- These techniques **find a meaningful subspace** where data still tells the same story.
###  Benefits:
- Faster computations
- Easier visualizations
- Simple models
- Better generalization
*End of Document*

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4c415a-4655-4ac4-9614-77a7a74f3e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (2.140.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-python-client) (2.33.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-python-client) (2.19.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ebf55097-3957-4204-bed7-83afd72df8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in w:\\hadoop-3.4.0\\logs\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "30e7fe11-da4d-47c8-bec8-b97bde144f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "#for interacting with the YouTube Api\n",
    "from googleapiclient.discovery import build \n",
    "import pandas as pd #for handling and saving data in csv format\n",
    "import time #For adding delays to avoid hitting API rate limits\n",
    "from datetime import datetime  #for handling date and time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "776ac131-7bba-42d2-8d9c-da9514cce3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize Youtube API\n",
    "\n",
    "API_KEY= 'AIzaSyDpjW5CYUAp497lU-HXM6AzAHQuBv3ooDU'\n",
    "\n",
    "#Initialize Youtube API\n",
    "youtube=build('youtube','v3',developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a1e395c0-88c5-470b-b94d-1fee87d77fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page with token: None\n",
      "Error fetching data:Missing required parameter \"part\"\n",
      "Failed to fetch data. Exiting.\n",
      "Saved 0 records to 'youtube_trending_videos.csv'\n"
     ]
    }
   ],
   "source": [
    "#Function to Fetch Trending videos\n",
    "def fetch_trending_videos(region_code='US',max_results=50,page_token=None):\n",
    "    try:\n",
    "        #create a request to fetch trending videos\n",
    "        request=youtube.videos().list()(\n",
    "            part='snippet,statistics,contentDetails,status',\n",
    "            chart='mostPopular',\n",
    "            regionCode=region_code,\n",
    "            maxResult=max_result,\n",
    "            pageToken=page_token)\n",
    "        #execute the request\n",
    "        response=request.execute()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data:{e}\")\n",
    "        return  None\n",
    "\n",
    "def main():\n",
    "    all_videos = []\n",
    "    next_page_token = None\n",
    "    max_results = 100  # Fetch 100 results per request\n",
    "\n",
    "    while len(all_videos) < 10000:\n",
    "        print(f\"Fetching page with token: {next_page_token}\")\n",
    "        response = fetch_trending_videos(max_results=max_results, page_token=next_page_token)\n",
    "\n",
    "        if response is None:\n",
    "            print(\"Failed to fetch data. Exiting.\")\n",
    "            break\n",
    "\n",
    "        for video in response.get('items', []):\n",
    "            video_data = {\n",
    "                'video_id': video['id'],\n",
    "                'trending_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'title': video['snippet']['title'],\n",
    "                'channel_title': video['snippet']['channelTitle'],\n",
    "                'category_id': video['snippet']['categoryId'],\n",
    "                'publish_time': video['snippet']['publishedAt'],\n",
    "                'tags': ','.join(video['snippet'].get('tags', [])),\n",
    "                'views': int(video['statistics'].get('viewCount', 0)),\n",
    "                'likes': int(video['statistics'].get('likeCount', 0)),\n",
    "                'dislikes': int(video['statistics'].get('dislikeCount', 0)),\n",
    "                'comment_count': int(video['statistics'].get('commentCount', 0)),\n",
    "                'thumbnail_link': video['snippet']['thumbnails']['high']['url'],\n",
    "                'comments_disabled': 'commentCount' not in video['statistics'],\n",
    "                'ratings_disabled': 'likeCount' not in video['statistics'] or 'dislikeCount' not in video['statistics'],\n",
    "                'video_error_or_removed': video['status'].get('uploadStatus') != 'processed',\n",
    "                'description': video['snippet']['description']\n",
    "            }\n",
    "            all_videos.append(video_data)\n",
    "        \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "        # Check if we have fetched enough records\n",
    "        if not next_page_token or len(all_videos) >= 10000:\n",
    "            break\n",
    "        \n",
    "        time.sleep(1)  # Avoid hitting rate limits\n",
    "\n",
    "    save_videos_to_csv(all_videos)\n",
    "    print(f\"Saved {len(all_videos)} records to 'youtube_trending_videos.csv'\")\n",
    "    # Run the Main Function\n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35fbc8-879b-4580-afd9-4762eb9cc4fe",
   "metadata": {},
   "source": [
    "**Explanation** : \n",
    "- **region_code** : The region for which you want to fetch trending videos (e.g., US for the United States).\n",
    "- **max_results**: Maximum number of results per API call (YouTube API allows up to 50 results per request).\n",
    "- **page_token**: Used for pagination to fetch additional pages of results.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "87c9112a-d335-443b-afaa-fae0f899363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save videos to csv\n",
    "def save_videos_to_csv(videos,filename='youtube_trending_videos.csv'):\n",
    "    df=pd.DataFrame(videos)\n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626a720-966d-47b1-a969-48c86e8c8870",
   "metadata": {},
   "source": [
    "**Explanation** :\n",
    "- **Videos** : A list of dictionaries containing video data.\n",
    "- **Filename** : The name of the csv file where the data will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d8656a8f-095a-44ab-9ca3-8de69bd76d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page with token: None\n",
      "Error fetching data:Missing required parameter \"part\"\n",
      "Failed to fetch data. Exiting.\n",
      "Saved 0 records to 'youtube_trending_videos.csv'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    all_videos = []\n",
    "    next_page_token = None\n",
    "    max_results = 100  # Fetch 100 results per request\n",
    "\n",
    "    while len(all_videos) < 10000:\n",
    "        print(f\"Fetching page with token: {next_page_token}\")\n",
    "        response = fetch_trending_videos(max_results=max_results, page_token=next_page_token)\n",
    "\n",
    "        if response is None:\n",
    "            print(\"Failed to fetch data. Exiting.\")\n",
    "            break\n",
    "\n",
    "        for video in response.get('items', []):\n",
    "            video_data = {\n",
    "                'video_id': video['id'],\n",
    "                'trending_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'title': video['snippet']['title'],\n",
    "                'channel_title': video['snippet']['channelTitle'],\n",
    "                'category_id': video['snippet']['categoryId'],\n",
    "                'publish_time': video['snippet']['publishedAt'],\n",
    "                'tags': ','.join(video['snippet'].get('tags', [])),\n",
    "                'views': int(video['statistics'].get('viewCount', 0)),\n",
    "                'likes': int(video['statistics'].get('likeCount', 0)),\n",
    "                'dislikes': int(video['statistics'].get('dislikeCount', 0)),\n",
    "                'comment_count': int(video['statistics'].get('commentCount', 0)),\n",
    "                'thumbnail_link': video['snippet']['thumbnails']['high']['url'],\n",
    "                'comments_disabled': 'commentCount' not in video['statistics'],\n",
    "                'ratings_disabled': 'likeCount' not in video['statistics'] or 'dislikeCount' not in video['statistics'],\n",
    "                'video_error_or_removed': video['status'].get('uploadStatus') != 'processed',\n",
    "                'description': video['snippet']['description']\n",
    "            }\n",
    "            all_videos.append(video_data)\n",
    "        \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "        # Check if we have fetched enough records\n",
    "        if not next_page_token or len(all_videos) >= 10000:\n",
    "            break\n",
    "        \n",
    "        time.sleep(1)  # Avoid hitting rate limits\n",
    "\n",
    "    save_videos_to_csv(all_videos)\n",
    "    print(f\"Saved {len(all_videos)} records to 'youtube_trending_videos.csv'\")\n",
    "    # Run the Main Function\n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae47066-f484-4615-b432-1ff2bed6bd0a",
   "metadata": {},
   "source": [
    "**Explanation** : \n",
    "- **all_video**s: A list to store the video data.\n",
    "- **max_results**: Set to 100 to fetch 100 videos per API request.\n",
    "- **while loop**: Continues fetching videos until 10,000 records are collected or no more pages are available.\n",
    "- **video_data**: A dictionary to store details about each video, including video ID, title, views, likes, dislikes, and other relevant fields.\n",
    "- **save_videos_to_csv(all_videos)**: Saves the collected video data to a CSV file once the loop completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "50558709-f0b7-4a17-b958-6ba318daaa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page with token: None\n",
      "Error fetching data:Missing required parameter \"part\"\n",
      "Failed to fetch data. Exiting.\n",
      "Saved 0 records to 'youtube_trending_videos.csv'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4bd00a-6405-4f6e-8ce2-d882d8a981b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
